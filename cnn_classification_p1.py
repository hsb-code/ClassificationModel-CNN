# -*- coding: utf-8 -*-
"""CNN_Classification_P1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x2wgysJaFtTHGu2rhthbLEuPsiGafMTy

# Import Dependencies
"""

import numpy as np
import tensorflow as tf
import keras as kr
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
import os
import cv2 # to read the image
import imghdr # to check if the file is an image
import matplotlib.pyplot as plt

# what is os?

# In Python, os is a built-in module that provides a way to interact with the operating system. It allows you
# to perform various tasks related to the file system, environment variables, and system-specific functions.

os.path.join('DATASET', 'Organic')
os.listdir('/content/drive/MyDrive/Colab Notebooks/Waste_DS_Train')

"""# Remove Dodgy Images"""

# Access Directory
data_dir = '/content/drive/MyDrive/Colab Notebooks/Waste_DS_Train'

# or you can give the path
# path = 'D:\ML&DL Projects\Waste_DS_Train'

# List all the formats
image_ext = ['jpeg', 'jpg', 'png', 'bmp']
image_ext

# checking folders in the directory
os.listdir(data_dir)

# accessing files from subfolders
os.listdir(os.path.join(data_dir, 'Organic'))

for image_class in os.listdir(data_dir):
    print(image_class)
    for image in os.listdir(os.path.join(data_dir, image_class)):
        print(image)
        image_path = os.path.join(data_dir, image_class, image)
        try:
            img = cv2.imread(image_path)
            tip = imghdr.what(image_path)
            if tip not in  image_ext:
                print('Not an image')
                os.remove(image_path)
        except Exception as e:
            print('Issue with file: ', format(image_path))

img = cv2.imread(os.path.join(data_dir, 'Organic', 'O_1.jpg'))
img.shape

"""# Printing Images"""

plt.imshow(img)

plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
# opencv reads image in BGR and matplotlib reads image in RGB
# so convert BGR to RGB

plt.show()

"""# Loading Data"""

# tf.data API is used to build pipelines to feed data into your model during training and inference.
# it simplifies the process of loading, processing, and managing large datasets for training deep learning models.
# A data pipeline is a series of data processing steps that involve the collection, transformation,
# and movement of data from one or more sources to one or more destinations.

# we will use keras image dataset from directory to load the data
# tf.keras.utils.image_dataset_from_directory('Waste_DS_Train') is a convenient utility provided by
# TensorFlow and Keras to quickly create a dataset from image files organized in directories. While it
# uses the tf.data API under the hood, it is specifically designed for loading image datasets, making it
# more user-friendly for common image classification tasks.

tf.data.Dataset??

tf.keras.utils.image_dataset_from_directory??

data = tf.keras.utils.image_dataset_from_directory(data_dir)
# this creates date pipeline
# this is just a generator
# this will not allow you to print data
# for that purpose you have to convert it into numpy array

data_iterator = data.as_numpy_iterator()
# now this will allow to access that pipeline or generator
# this is allowing to loop through

batch = data_iterator.next()
# this will give the first batch of images ana labels
# if you want to access the next batch then again you have to use data_iterator.next()

print(len(batch)) # it should be 2 (one for images and one for label)
print(batch) # this will print batch of images and labels
print(batch[0].shape) # this will total images in batch, their shape and dimension

batch[1]

"""Check for assigned classes"""

fig, ax = plt.subplots(ncols = 4, figsize=(20, 20))
for idx, img in enumerate(batch[0][:4]):
    ax[idx].imshow(img.astype('int'))
    ax[idx].title.set_text(batch[1][idx])
# The enumerate function in Python is used to loop over an iterable (such as a list, tuple, or string)
# while keeping track of the index of the current item.

"""Organic Class = 1 & Inorganic Class = 0

# Data Pre-Processing

1. Scale
2. Split
"""

# for image data we need to do pre-process by scaling image values between 0 and 1 instead of 0 and 255
# this helps our deep learning model generalize faster and produce better results

# we also split our data into training, validation and test sets to avoid overfitting

print(batch[0].shape)
print(batch[0].min())
print(batch[0].max())

# when we load data in image representation then it is in the form of 0 to 255
# so we have to scale it between 0 and 1
# when you are dealing with deep learning model you should scale data it help optimization a ton faster

"""Data Scaling"""

# we can scale data by dividing it by 255
# scaled = batch[0]/255
# but it will scale on that batch of data
# we need whole data to be scaled
# for that purpose we will use map function
# map function will apply a function to each element of the dataset

data = data.map(lambda x, y: (x/255, y))

# data: This is a dataset that contains pairs of (x, y), where x is an image data and y is a label associated
# with that image.

# map(): This is a method often used in programming to apply a given function to each element of a dataset
# (or list, array, etc.) and create a new dataset with the transformed values.

# lambda x, y: (x/255, y): This is a lambda function, which is an anonymous inline function that takes two
# arguments, x and y, and returns a tuple where the first element is the result of dividing x by 255 and the
# second element is y.

data.as_numpy_iterator().next()[0].min()

# direct check without assigning to variable
data.as_numpy_iterator().next()[0].max()

# check by assigning to variable
scaled_iterator = data.as_numpy_iterator()
batch = scaled_iterator.next()

print(batch[0].min())
print(batch[0].max())

fig, ax = plt.subplots(ncols = 4, figsize=(20, 20))
for idx, img in enumerate(batch[0][:4]):
    ax[idx].imshow(img) # removing type bcz data is no longer int after scaling
    ax[idx].title.set_text(batch[1][idx])

"""Data Splitting"""

len(data)

# 784 bactches and each batch with 32 images
# 784*32 = 25088 images

train_size = int(0.7*len(data))
val_size = int(0.15*len(data))+1
test_size = int(0.15*len(data))+1

# When you calculate the size of the validation set and the test set as a percentage of the total data,
# there may be cases where the percentage calculation results in a fractional number of data points.
# In such cases, rounding up to the nearest whole number (by adding 1) ensures that you have at least
# one data point in the validation set.

print(train_size)
print(val_size)
print(test_size)

train_size+val_size+test_size

train = data.take(train_size)
val = data.skip(train_size).take(val_size)
test = data.skip(train_size+val_size).take(test_size)

print(len(train))
print(len(val))
print(len(test))

"""# Deep Model

Model
"""

# tf.random.set_seed(1234)
# lambdas = [0.0, 0.001, 0.01, 0.05, 0.1, 0.2, 0.3]
# models=[None] * len(lambdas)

# for i in range(len(lambdas)):
#     lambda_ = lambdas[i]
#     models[i] = Sequential(
#         [
#             Conv2D(16, (3,3), 1, activation = 'relu', input_shape= (256,256,3), kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
#             # 16 filters, 3*3 kernel size, 1 stride
#             MaxPooling2D(),

#             Conv2D(32, (3,3), 1, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
#             MaxPooling2D(),

#             Conv2D(16, (3,3), 1, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
#             MaxPooling2D(),

#             # flatten dataset before using dense layer because dense layer takes 1D data
#             Flatten(),

#             Dense(256, activation = 'relu', kernel_regularizer=tf.keras.regularizers.l2(lambda_)),
#             Dense(1, activation = 'sigmoid', kernel_regularizer=tf.keras.regularizers.l2(lambda_))

#         ]
#     )

#     models[i].compile( optimizer=tf.keras.optimizers.Adam(0.01), loss= 'binary_crossentropy', metrics = ['accuracy'])

#     models[i].fit(train, epochs = 10, validation_data = val, callbacks = [tensorboard_callback])
#     print(f"Finished lambda = {lambda_}")

model = Sequential()
model.add(Conv2D(16, (3,3), 1, activation = 'relu', input_shape= (256,256,3), kernel_regularizer=tf.keras.regularizers.l2(0.001)))
# 16 filters, 3*3 kernel size, 1 stride
model.add(MaxPooling2D())

model.add(Conv2D(32, (3,3), 1, activation = 'relu',  kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(MaxPooling2D())

model.add(Conv2D(16, (3,3), 1, activation = 'relu',  kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(MaxPooling2D())

# flatten dataset before using dense layer because dense layer takes 1D data
model.add(Flatten())
model.add(Dropout(0.2))

model.add(Dense(256, activation = 'relu',  kernel_regularizer=tf.keras.regularizers.l2(0.001)))
model.add(Dense(1, activation = 'sigmoid',  kernel_regularizer=tf.keras.regularizers.l2(0.001)))

model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

model.summary()

"""Training"""

logdir = '/content/drive/MyDrive/Colab Notebooks/logs'

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir = logdir)

hist = model.fit(train, epochs = 15, validation_data = val, callbacks = [tensorboard_callback])

hist

hist.history

fig = plt.figure()
plt.plot(hist.history['loss'], color ='teal', label = 'loss')
plt.plot(hist.history['val_loss'], color = 'orange', label = 'val_loss')
fig.suptitle('Loss', fontsize = 20)
plt.legend(loc = 'upper right')
plt.show()

fig = plt.figure()
plt.plot(hist.history['accuracy'], color ='teal', label = 'accuracy')
plt.plot(hist.history['val_accuracy'], color = 'orange', label = 'val_accuracy')
fig.suptitle('Accuracy', fontsize = 20)
plt.legend(loc = 'upper right')
plt.show()

"""# Evaluate Performance"""

from tensorflow.keras.metrics import Precision, Recall, BinaryAccuracy

pre = Precision()
re = Recall()
acc = BinaryAccuracy()

len(test)

for batch in test.as_numpy_iterator():
  X, y = batch
  yhat = model.predict(X)
  pre.update_state(y, yhat)
  re.update_state(y, yhat)
  acc.update_state(y,yhat)

print(f'Precision: {pre.result().numpy()}, Recall: {re.result().numpy()}, Accuracy: {acc.result().numpy()}')

"""# Testing Model"""

import cv2
import matplotlib.pyplot as plt
img = cv2.imread('/content/drive/MyDrive/Colab Notebooks/food3.jpg')
plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
plt.show()

# Assuming 'img' is the BGR image
rgb_image = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

# Resize the RGB image
resize = tf.image.resize(rgb_image, (256, 256))

# Display the resized RGB image
plt.imshow(resize.numpy().astype(int))
plt.show()

# for testing, NN demads to upload a batch not a single image.
# so we need to expand dimensions of image

resize

resize.shape

np.expand_dims(resize, 0)

np.expand_dims(resize, 0).shape

yhat = model.predict(np.expand_dims(resize/255, 0)) # increasing dimensions and scaling

yhat

if yhat > 0.5:
  print('The waste is Organic')
else:
  print('The waste is inoganic')

"""# Save the Model"""

from tensorflow.keras.models import load_model

model.save(os.path.join('/content/drive/MyDrive/Colab Notebooks/Models', 'Waste_Classification.h5'))

"""# Reloading"""

new_model = load_model(os.path.join('/content/drive/MyDrive/Colab Notebooks/Models', 'Waste_Classification.h5'))

new_model

# now you can make predictions by just loading your new_model

yhat_new_model = new_model.predict(np.expand_dims(resize/255, 0))

yhat_new_model

if yhat_new_model > 0.5:
  print('The waste is Organic')
else:
  print('The waste is inoganic')

"""# The End"""